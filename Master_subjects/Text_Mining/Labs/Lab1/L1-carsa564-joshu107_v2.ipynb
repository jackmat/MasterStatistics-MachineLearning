{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Students:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Get the webpage content by using functions in \n",
    "__[urllib module](https://docs.python.org/3/library/urllib.html#module-urllib)__.\n",
    "\n",
    "Other libraries are also fine to achieve the crawling.\n",
    "\n",
    "e.g. scrapy, beautifulsoup... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://play.google.com/store/apps/top\"\n",
    "shortUrl =  \"https://play.google.com\"\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import re , numpy\n",
    "\n",
    "contents = urllib.request.urlopen(url).read().decode('utf8') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Get app url by regular expression using functions from __[re module](https://docs.python.org/3/library/re.html?highlight=re#module-re)__.\n",
    "\n",
    "A useful online regular expression check.\n",
    "__[Check your regular expression first](https://regex101.com)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mylist = numpy.unique([re.findall(pattern = \"href=\\\"/store/apps/details.*?\\\"\",\n",
    "                                  string =contents)]) #Getting the link reference\n",
    "wholeUrl = [str(shortUrl + link[6:-1] + \"&hl=en\")  for link in mylist]\n",
    "##Doing the same for each link to get more links\n",
    "contents2 = [urllib.request.urlopen(link).read().decode('utf8') for link in wholeUrl]\n",
    "\n",
    "newlist2 =[]\n",
    "for link in contents2:\n",
    "    newlist2= newlist2 + re.findall(pattern = \"href=\\\"/store/apps/details.*?\\\"\",\n",
    "                                    string =link)   #Getting the link reference\n",
    "\n",
    "newlist2 = numpy.unique(newlist2)\n",
    "wholeUrl2 = [str(shortUrl + link[6:-1] + \"&hl=en\")  for link in newlist2 \n",
    "             if not \"reviewId\" in link]\n",
    "wholeUrl2 = numpy.unique(wholeUrl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Access specific webpage to get description of each app and then store the description in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "appname =[]\n",
    "descriptionRes=[]\n",
    "for link in wholeUrl2[0:1500]:\n",
    "    with urlopen(link) as url:\n",
    "        descriptionRes = descriptionRes + re.findall(pattern = \"itemprop=\\\"description.*?\\\">.*?<div jsname=\\\".*?\\\">.*?</div>\", string = url.read().decode('utf8'))\n",
    "    with urlopen(link) as url:    \n",
    "        appname= appname + re.findall(\"<title id=\\\"main-title\\\">.*? - Android\", string = url.read().decode('utf8'))\n",
    "\n",
    "cleanDes= [link[58:-6] for link in descriptionRes]\n",
    "appname = [link[23:-10] for link in appname]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Inverted file index (Vector Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Preprocess text using NLP techniques from __[nltk module](http://www.nltk.org/py-modindex.html)__.\n",
    "\n",
    "Using nltk.download(ID) to get the corpora if it is not downloaded before. __[nltk corpora](http://www.nltk.org/nltk_data/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tf(documents):\n",
    "    WordList =[]\n",
    "    \n",
    "    ##Creating the list of words of all documents being WordList\n",
    "    for document in documents:\n",
    "        splitWords = document.split(\" \")\n",
    "        for Word in splitWords:\n",
    "            if Word not in WordList:\n",
    "                WordList.append(Word)\n",
    "    \n",
    "    ##Matrix with colnames(Documents), rownames = Words            \n",
    "    tfperdoc = np.zeros([len(WordList), len(documents)])\n",
    "    for i in range(len(documents)): #### CHange from here\n",
    "        splitWords = documents[i].split(\" \")\n",
    "        for Word in splitWords:\n",
    "            tfperdoc[WordList.index(Word), i] +=1\n",
    "    ##Dividing by the Columnsum and then divide by the larger term in the column\n",
    "    for column in range(len(documents)):\n",
    "        tfperdoc [:,column] = tfperdoc [:,column]/np.sum(tfperdoc [:,column])\n",
    "        tfperdoc [:,column] = tfperdoc [:,column]/np.amax(tfperdoc [:,column])\n",
    "\n",
    "    return [tfperdoc, WordList]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def idf(tfres):\n",
    "    #Number of documents in U that contain t\n",
    "    nwords = tfres.shape[0]\n",
    "    ndocs = tfres.shape[1]\n",
    "    wordDoc = np.zeros([nwords, ndocs])\n",
    "    WordCount = np.zeros([nwords])\n",
    "    for i in range(nwords):\n",
    "        for j in range(ndocs):\n",
    "            if(tfres[i,j]==0):\n",
    "                wordDoc[i,j]=0\n",
    "            else:    \n",
    "                wordDoc[i,j]=1\n",
    "    for row in range(nwords):\n",
    "        WordCount[row] = np.sum(wordDoc[row,:])\n",
    "\n",
    "    for number in range(nwords):\n",
    "        if WordCount[number] == 0:\n",
    "            WordCount[number] = 0\n",
    "        else:\n",
    "            WordCount[number] = ndocs/WordCount[number]\n",
    "            WordCount[number] = np.log(WordCount[number])\n",
    "    return WordCount\n",
    "\n",
    "\n",
    "def tfidf(tf, idf):\n",
    "    ndocs = tf.shape[1]\n",
    "    for column in range(ndocs):\n",
    "        tf[:,column] = tf[:,column]*idf\n",
    "        \n",
    "    return(tf)\n",
    "\n",
    "\n",
    "\n",
    "def tfidfquery(query, Matwordlist, tf):\n",
    "    \n",
    "    queryWordList =[]\n",
    "    queryWordCount =[]\n",
    "    ##Creating the list of words with the query being WordList\n",
    "    \n",
    "    splitWords = query.split(\" \")\n",
    "    for Word in splitWords:\n",
    "        if Word not in queryWordList:\n",
    "            queryWordList.append(Word)\n",
    "            queryWordCount.append(1)\n",
    "        else: \n",
    "            queryWordCount[queryWordList.index(Word)] +=1\n",
    "    \n",
    "    queryWordList =np.asarray(queryWordList)\n",
    "    queryWordCount =np.asarray(queryWordCount)\n",
    "    \n",
    "    ## Matrix of length of vectorwords\n",
    "    Matrix_tf = np.zeros([len(queryWordList),tf.shape[1]])\n",
    "    for i in range(len(queryWordList)):\n",
    "        if queryWordList[i] in Matwordlist:\n",
    "            Matrix_tf[i,] = tf[Matwordlist.index(queryWordList[i]),] \n",
    "    ##Dividing by the Columnsum and then divide by the larger term in the column\n",
    "    if np.array_equal(Matrix_tf ,np.zeros([len(queryWordList),tf.shape[1]])):\n",
    "        return \"No match found\"\n",
    "    else:\n",
    "        queryWordCount  = queryWordCount/np.sum(queryWordCount)\n",
    "        queryWordCount = queryWordCount/np.amax(queryWordCount)  #query word = tf now\n",
    "    ##Total tf\n",
    "    Alltf = np.c_[Matrix_tf, queryWordCount] # insert values before column 3\n",
    "    ###Doing idf    \n",
    "    Allidf= idf(Alltf)\n",
    "    ##tfidif =(0.5+0.5tf)*idf\n",
    "    tfidfres = 0.5*Matrix_tf + 0.5*tfidf(Matrix_tf,Allidf)\n",
    "    return [tfidfres, queryWordCount.reshape(-1,1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "clean = [re.sub(r'[^a-zA-Z0-9\\[\\]]',' ' ,link).lower() for link in cleanDes] #Clean and tokenize \n",
    "\n",
    "###Part 2\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "\n",
    "def remove_nonalphanum(words):\n",
    "    return [re.sub(r'[^a-zA-Z0-9\\[\\]]',' ' , word) for word in words]\n",
    "\n",
    "def tokenize(words):\n",
    "    tknzr = TweetTokenizer()\n",
    "    return [tknzr.tokenize(word) for word in words]\n",
    "\n",
    "def lowercase(words):\n",
    "    return [[word.lower() for word in lines] for lines in words]\n",
    "\n",
    "def stemming(words):\n",
    "    st = LancasterStemmer()\n",
    "    return [[st.stem(word) for word in lines] for lines in words]\n",
    "\n",
    "def stoppingwords(wordes):\n",
    "    stop= set(stopwords.words('english'))\n",
    "    stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', 'br']) # remove it if you need punctuation \n",
    "    return [[word for word in lines if word not in stop] for lines in wordes]\n",
    "\n",
    "def checkLanguage(data):\n",
    "    checkWords = set(words.words())\n",
    "    return [[word for word in lines if word in checkWords] for lines in data]\n",
    "    \n",
    "def joiner(words):\n",
    "    return [\" \".join(word) for word in words] \n",
    "\n",
    "def processdata(data):\n",
    "    functionList = [remove_nonalphanum, tokenize, lowercase, stoppingwords, checkLanguage ,stemming , joiner]\n",
    "    for function in functionList:\n",
    "        data = function(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def QueryProcess(query, NtopResults, dictionary):\n",
    "    Description = list(dictionary.values())\n",
    "    appnames = list(dictionary.keys())\n",
    "    \n",
    "    k = NtopResults # Number of results to show\n",
    "    #####################   Matrix numerical transformation \n",
    "    Result = processdata(Description) ##Processing data calling function from part2\n",
    "    tf1=tf(Result)\n",
    "    idf1= idf(tf1[0])\n",
    "    tfidfMatrix=tfidf(tf1[0], idf1)       \n",
    "    \n",
    "    #Query numerical transformation\n",
    "    query = processdata([query])[0]###Processing query calling function from part2\n",
    "    tfidfquest=tfidfquery(query, tf1[1], tf1[0])    \n",
    "    \n",
    "    ###Similarity comparison between matrix and query\n",
    "    search_result = cosine_similarity( tfidfquest[0].transpose(),tfidfquest[1].transpose()).transpose()\n",
    "    \n",
    "    SearchNiceArray= search_result.reshape(1,search_result .size)[0]\n",
    "    kTopIndex = SearchNiceArray.argsort(axis = 0 )[::-1][:k].flatten()\n",
    "    print([SearchNiceArray[i] for i in kTopIndex ])\n",
    "    k_recomendations = [appnames[i] for i in kTopIndex]\n",
    "    return k_recomendations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...)Compute tdidf using functions from __[scikit-learn module](http://scikit-learn.org/stable/modules/classes.html)__.\n",
    "\n",
    "eg. TfidfVectorizer is used for converting a collection of raw documents to a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mydict=dict(zip(appname, clean))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Process\n",
    "\n",
    "eg. \"Dragon, Control, hero, running\"\n",
    "\n",
    "eg. \"The hero controls the dragon to run.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99943677616527915, 0.99787924279608409, 0.99752227309768959, 0.99080533186017461, 0.9908053318601745, 0.9908053318601745, 0.97940201123711323, 0.97940201123711323, 0.97940201123711323, 0.97940201123711323]\n",
      "['LOVOO', 'Paktor: Meet New People', 'DOWN Dating: Match, Chat, Date', 'Driving Theory Test for Cars 2018', 'OkCupid Dating', 'MyPlate Calorie Tracker', 'Saudi Driving Test - Dallah', 'Calorie Counter - EasyFit free', 'Jaumo Flirt Chat &amp; Dating', 'Summoners War']\n"
     ]
    }
   ],
   "source": [
    "myquery1= \"dating with people\"\n",
    "myquery2= \"Work out fitness\"\n",
    "myquery3= \"Cooking food for old people\"\n",
    "myquery4= \"wrk ot fitnss\"\n",
    "myquery5= \"Capture monster with a ball\"\n",
    "\n",
    "numberRecommendations= 10\n",
    "\n",
    "print(QueryProcess(myquery1, numberRecommendations, Mydict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that it is capturing the \"dating\" word because it prints out relevant apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9988337856652223, 0.98847592334783863, 0.9884759233478384, 0.96051567714137609, 0.93613862503187883, 0.90750634270384523, 0.90750634270384511, 0.90750634270384511, 0.907506342703845, 0.907506342703845]\n",
      "['Sandbox Number Coloring Book - Color By Number', 'Lose It! - Calorie Counter', 'StrongLifts 5x5 Workout Gym Log &amp; Personal Trainer', 'Calorie Counter - MyNetDiary', 'JEFIT: Workout Tracker, Gym Log &amp; Personal Trainer', 'Abs workout A6W', 'myMail – Email for Hotmail, Gmail and Outlook Mail', 'Fire Emblem Heroes', '30 Day Ab Challenge', '100 GB Free Cloud Drive from Degoo']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery2, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the first results are what we are looking for, because work out is separate, it also takes into account matches for \"work\" leading to results like googledocs. The data process should be improved in order to suggest not only \"work\" \"out\" and  workout together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.830253794045271, 0.79476003826774844, 0.7471311194301018, 0.7068446797289778, 0.70377686166340414, 0.70293071793689754, 0.68871633846212799, 0.68871633846212799, 0.68871633846212776, 0.68871633846212776]\n",
      "['Village and Farm', 'Toca Life: Office', 'Calorie Counter - MyFitnessPal', 'Wormax.io', 'Farm On!', 'STRIKERS 1945-2', 'WordOn: multiplayer word game', \"What's the Difference?\", 'Sites in VR', 'Word Search Online']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery3, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are evaluating two topics at the same time, but in fact the algorithm should be taking into account ideally the semantic meaning of the sentence (e.g. looking only for cooking food and giving less importance to old people). It can be seen that it finds some links matching to recipies, but others are more random like farming or messenger. Maybe the cosine similarity is low, but since we are not printing it, we don 't know how similar these last matches are. We should set a minimum threshold on our cosine similarity before returning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "['王國征戰', 'اخترع Amino', 'Castle Clash: فريق الشجعان', 'كويز كلاش', 'Discovery and Invention in Hindi', 'N.O.V.A. Legacy', 'Dragon Mania Legends', 'Sniper Fury: Top shooter -fun shooting games - FPS', 'Green Farm 3', 'March of Empires: War of Lords']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery4, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is trying misspellings: It can be seen that it does not deal with misspellings. Improvements for that should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81578660436508832, 0.81578660436508832, 0.80824868062472321, 0.77110116631557823, 0.74322764910004158, 0.57735026918962584, 0.57735026918962584, 0.57735026918962584, 0.57735026918962584, 0.57735026918962584]\n",
      "['March of Empires: War of Lords', 'Golf Star™', 'World War III: European Wars', 'Balls Bounce', 'Toy Defense Fantasy - TD Strategy Game', 'Defenders 2: Tower Defense CCG', 'Basketball Stars', 'New Subway Surf: Bus Hours 2018', 'Geo dash  ball 4 : in 3 world', 'Last Hope TD - Zombie Tower Defense with Heroes']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery5, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this one I was trying to get back the pokemon app. Instead, we got some other apps that to some extent are maybe more  suitable on their description, but that they are less relevant than others in terms of trendiness. Some weigths should be put inside the algorithm to give importance also to this feature. Also, our description may not be fitting pokemon's description.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
