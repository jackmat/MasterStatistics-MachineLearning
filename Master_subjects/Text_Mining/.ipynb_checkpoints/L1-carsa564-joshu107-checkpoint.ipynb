{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Students:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Get the webpage content by using functions in \n",
    "__[urllib module](https://docs.python.org/3/library/urllib.html#module-urllib)__.\n",
    "\n",
    "Other libraries are also fine to achieve the crawling.\n",
    "\n",
    "e.g. scrapy, beautifulsoup... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://play.google.com/store/apps/top\"\n",
    "shortUrl =  \"https://play.google.com\"\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import urllib\n",
    "import re , numpy\n",
    "\n",
    "contents = urllib.request.urlopen(url).read().decode('utf8') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Get app url by regular expression using functions from __[re module](https://docs.python.org/3/library/re.html?highlight=re#module-re)__.\n",
    "\n",
    "A useful online regular expression check.\n",
    "__[Check your regular expression first](https://regex101.com)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = numpy.unique([re.findall(pattern = \"href=\\\"/store/apps/details.*?\\\"\",\n",
    "                                  string =contents)]) #Getting the link reference\n",
    "wholeUrl = [str(shortUrl + link[6:-1] + \"&hl=en\")  for link in mylist]\n",
    "##Doing the same for each link to get more links\n",
    "contents2 = [urllib.request.urlopen(link).read().decode('utf8') for link in wholeUrl]\n",
    "\n",
    "newlist2 =[]\n",
    "for link in contents2:\n",
    "    newlist2= newlist2 + re.findall(pattern = \"href=\\\"/store/apps/details.*?\\\"\",\n",
    "                                    string =link)   #Getting the link reference\n",
    "\n",
    "newlist2 = numpy.unique(newlist2)\n",
    "wholeUrl2 = [str(shortUrl + link[6:-1] + \"&hl=en\")  for link in newlist2 \n",
    "             if not \"reviewId\" in link]\n",
    "wholeUrl2 = numpy.unique(wholeUrl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Access specific webpage to get description of each app and then store the description in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "appname =[]\n",
    "descriptionRes=[]\n",
    "for link in wholeUrl2[0:1500]:\n",
    "    with urlopen(link) as url:\n",
    "        descriptionRes = descriptionRes + re.findall(pattern = \"itemprop=\\\"description.*?\\\">.*?<div jsname=\\\".*?\\\">.*?</div>\", string = url.read().decode('utf8'))\n",
    "    with urlopen(link) as url:    \n",
    "        appname= appname + re.findall(\"<title id=\\\"main-title\\\">.*? - Android\", string = url.read().decode('utf8'))\n",
    "\n",
    "cleanDes= [link[58:-6] for link in descriptionRes]\n",
    "appname = [link[23:-10] for link in appname]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Inverted file index (Vector Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Preprocess text using NLP techniques from __[nltk module](http://www.nltk.org/py-modindex.html)__.\n",
    "\n",
    "Using nltk.download(ID) to get the corpora if it is not downloaded before. __[nltk corpora](http://www.nltk.org/nltk_data/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "clean = [re.sub(r'[^a-zA-Z0-9\\[\\]]',' ' ,link).lower() for link in cleanDes] #Clean and tokenize \n",
    "\n",
    "###Part 2\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "\n",
    "def remove_nonalphanum(words):\n",
    "    return [re.sub(r'[^a-zA-Z0-9\\[\\]]',' ' , word) for word in words]\n",
    "\n",
    "def tokenize(words):\n",
    "    tknzr = TweetTokenizer()\n",
    "    return [tknzr.tokenize(word) for word in words]\n",
    "\n",
    "def lowercase(words):\n",
    "    return [[word.lower() for word in lines] for lines in words]\n",
    "\n",
    "def stemming(words):\n",
    "    st = LancasterStemmer()\n",
    "    return [[st.stem(word) for word in lines] for lines in words]\n",
    "\n",
    "def stoppingwords(wordes):\n",
    "    stop= set(stopwords.words('english'))\n",
    "    stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', 'br']) # remove it if you need punctuation \n",
    "    return [[word for word in lines if word not in stop] for lines in wordes]\n",
    "\n",
    "def checkLanguage(data):\n",
    "    checkWords = set(words.words())\n",
    "    return [[word for word in lines if word in checkWords] for lines in data]\n",
    "    \n",
    "def joiner(words):\n",
    "    return [\" \".join(word) for word in words] \n",
    "\n",
    "def processdata(data):\n",
    "    functionList = [remove_nonalphanum, tokenize, lowercase, stoppingwords, checkLanguage ,stemming , joiner]\n",
    "    for function in functionList:\n",
    "        data = function(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def QueryProcess(query, NtopResults, dictionary):\n",
    "    Description = list(dictionary.values())\n",
    "    appnames = list(dictionary.keys())\n",
    "    \n",
    "    k = NtopResults # Number of results to show\n",
    "    #####################   Matrix numerical transformation \n",
    "    Result = processdata(Description) ##Processing data calling function from part2\n",
    "    transvector = TfidfVectorizer()\n",
    "    tf_matrix = transvector.fit_transform(Result)\n",
    "    \n",
    "    #Query numerical transformation\n",
    "    query = processdata([query])###Processing query calling function from part2\n",
    "    query = transvector.transform(query)\n",
    "    \n",
    "    ###Similarity comparison between matrix and query\n",
    "    search_result = cosine_similarity(tf_matrix, query) \n",
    "    SearchNiceArray= search_result.reshape(1,search_result .size)[0]\n",
    "    kTopIndex = SearchNiceArray.argsort(axis = 0 )[::-1][:k].flatten()\n",
    "    k_recomendations = [appnames[i] for i in kTopIndex ]\n",
    "    return k_recomendations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...)Compute tdidf using functions from __[scikit-learn module](http://scikit-learn.org/stable/modules/classes.html)__.\n",
    "\n",
    "eg. TfidfVectorizer is used for converting a collection of raw documents to a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mydict=dict(zip(appname, clean))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Process\n",
    "\n",
    "eg. \"Dragon, Control, hero, running\"\n",
    "\n",
    "eg. \"The hero controls the dragon to run.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cinemaniac - Movies To Watch', 'POF Free Dating App', 'DOWN Dating: Match, Chat, Date', 'QuackQuack Dating App', 'Once - Handpicked Matches Every day', 'Mental Calculation', 'Fem - Lesbian Dating Chat App', 'Messenger Lite: Free Calls &amp; Messages', 'Klarna - Smoooth Payments', 'Flash Alerts 2']\n"
     ]
    }
   ],
   "source": [
    "myquery1= \"dating\"\n",
    "myquery2= \"Work out fitness\"\n",
    "myquery3= \"Cooking food for old people\"\n",
    "myquery4= \"wrk ot fitnss\"\n",
    "myquery5= \"Capture monster with a ball\"\n",
    "\n",
    "numberRecommendations= 10\n",
    "\n",
    "print(QueryProcess(myquery1, numberRecommendations, Mydict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that it is capturing the \"dating\" word because it prints out relevant apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atari Fit™', '30 Day Abs Workout Challenge', '30 Day Ab Challenge', 'Audio Recorder', 'Reckless Getaway 2', 'Microsoft OneDrive', 'Fit In The Hole', '100 GB Free Cloud Drive from Degoo', 'Google Docs', 'War Robots']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery2, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the first results are what we are looking for, because work out is separate, it also takes into account matches for \"work\" leading to results like googledocs. The data process should be improved in order to suggest not only \"work\" \"out\" and  workout together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Toca Kitchen 2', 'Monster Legends - RPG', 'Village and Farm', 'QuedamosdeTapas', 'ikman - Sell, Buy &amp; Find Jobs', 'My Cafe: Recipes &amp; Stories - World Cooking Game', 'Toca Lab: Elements', 'Facebook Mentions', 'Village Life: Love &amp; Babies', 'Messenger Lite: Free Calls &amp; Messages']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery3, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are evaluating two topics at the same time, but in fact the algorithm should be taking into account ideally the semantic meaning of the sentence (e.g. looking only for cooking food and giving less importance to old people). It can be seen that it finds some links matching to recipies, but others are more random like farming or messenger. Maybe the cosine similarity is low, but since we are not printing it, we don 't know how similar these last matches are. We should set a minimum threshold on our cosine similarity before returning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quizduell', 'Bubble Bust 2 - Pop Bubble Shooter', 'Little Big City 2', 'World at Arms', 'N.O.V.A. Legacy', 'Order &amp; Chaos 2: 3D MMO RPG', 'Real Football', 'MARVEL Spider-Man Unlimited', 'UNO ™ &amp; Friends', 'Euro Farm Simulator: Beetroot']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery4, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is trying misspellings: It can be seen that it does not deal with misspellings. Improvements for that should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facemoji Emoji Keyboard - Cute Emoji,Theme,Sticker', 'FlashScore - sportresultat', 'Magnetic balls bubble shoot', 'Balls Bounce', 'Monster Busters: Link Flash', 'Equilibrians', 'Equilibrians (Lite)', 'Geo dash  ball 4 : in 3 world', 'Ball Hop King', 'DoggCatcher Podcast Player']\n"
     ]
    }
   ],
   "source": [
    "print(QueryProcess(myquery5, numberRecommendations, Mydict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this one I was trying to get back the pokemon app. Instead, we got some other apps that to some extent are maybe more  suitable on their description, but that they are less relevant than others in terms of trendiness. Some weigths should be put inside the algorithm to give importance also to this feature. Also, our description may not be fitting pokemon's description.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
